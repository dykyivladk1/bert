{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import math\n",
    "import re\n",
    "from random import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Hey there, how’s it going? I’m Alex.\\n'\n",
    "    'Hi Alex! I’m Taylor. Nice to meet you.\\n'\n",
    "    'Nice to meet you too. How’s your day been?\\n'\n",
    "    'Pretty good, thanks for asking. How about yours?\\n'\n",
    "    'It’s been busy, but good overall.\\n'\n",
    "    'That’s great to hear. I just finished a painting.\\n'\n",
    "    'Wow, that’s awesome, Taylor! I’d love to see it sometime.\\n'\n",
    "    'Sure thing! Maybe next time we meet.\\n'\n",
    "    'Speaking of which, are you free this weekend?\\n'\n",
    "    'I think so. Do you have plans?\\n'\n",
    "    'I was thinking of checking out that new cafe downtown.\\n'\n",
    "    'That sounds fun! Count me in.\\n'\n",
    "    'Great, let’s meet there at 2 PM on Saturday.\\n'\n",
    "    'Perfect! I’ll see you then, Alex.\\n'\n",
    "    'See you, Taylor. Have a good night!\\n'\n",
    "    'You too, Alex. Good night!\\n'\n",
    "    'On Saturday, Alex and Taylor met at the cafe.\\n'\n",
    "    'This place has such a cozy vibe, doesn’t it?\\n'\n",
    "    'Definitely. The decor is really nice too.\\n'\n",
    "    'Yeah, and the coffee smells amazing.\\n'\n",
    "    'They enjoyed their drinks and chatted for hours.\\n'\n",
    "    'Thanks for hanging out today, Taylor.\\n'\n",
    "    'No problem, Alex. I had a blast.\\n'\n",
    "    'Let’s do this again soon.\\n'\n",
    "    'For sure! I’m looking forward to it.\\n'\n",
    "    'Until next time, Taylor.\\n'\n",
    "    'Until next time, Alex.'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('conversation.json', 'r') as file:\n",
    "    text_lines = json.load(file)\n",
    "text = '\\n'.join(text_lines) \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.sub('[.,!?\\\\-]', '', text.lower()).split('\\n')\n",
    "word_list = list(set(' '.join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "\n",
    "number_dict = {i:w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "\n",
    "token_list = list()\n",
    "\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "batch_size = 6\n",
    "n_layers = 6\n",
    "n_heads = 12\n",
    "d_model = 768\n",
    "d_ff = 768 * 4\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "n_segments = 2\n",
    "\n",
    "max_pred = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sentence_pair(sentences, token_list):\n",
    "    tokens_a_index = randrange(len(sentences))\n",
    "    tokens_b_index = randrange(len(sentences))\n",
    "    tokens_a = token_list[tokens_a_index]\n",
    "    tokens_b = token_list[tokens_b_index]\n",
    "    return tokens_a, tokens_b, tokens_a_index, tokens_b_index\n",
    "\n",
    "\n",
    "sentences = [\"The cat sat on the mat.\", \"The dog barked at the cat.\", \"The bird sang a song.\"]\n",
    "token_list = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14]]\n",
    "\n",
    "\n",
    "tokens_a, tokens_b, tokens_a_index, tokens_b_index = select_random_sentence_pair(sentences, token_list)\n",
    "print(tokens_a, tokens_b, tokens_a_index, tokens_b_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_segment_ids(tokens_a, tokens_b, word_dict):\n",
    "    input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "    segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "    # segment ids represents which tokens belong to the first \n",
    "    # sentence and which belong to the second sentence\n",
    "    # 0 first sentence 1 second sentence\n",
    "    return input_ids, segment_ids\n",
    "\n",
    "input_ids, segment_ids = construct_input_segment_ids(tokens_a,\n",
    "                                                     tokens_b,\n",
    "                                                     word_dict)\n",
    "\n",
    "\n",
    "print(input_ids, '\\n', segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(input_ids, word_dict, max_pred, vocab_size, number_dict):\n",
    "    n_pred = min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n",
    "    cand_makes_pos = [i for i, token in enumerate(input_ids) if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "    shuffle(cand_makes_pos)\n",
    "\n",
    "    masked_tokens, masked_pos = [], []\n",
    "    for pos in cand_makes_pos[:n_pred]:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random() < 0.8:\n",
    "            input_ids[pos] = word_dict['[MASK]']\n",
    "        elif random() < 0.5:\n",
    "            index = randint(0, vocab_size - 1)\n",
    "            input_ids[pos] = word_dict[number_dict[index]]\n",
    "    \n",
    "    return masked_tokens, masked_pos, input_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "masked_tokens, masked_pos, input_ids = mask_tokens(\n",
    "    input_ids, word_dict, max_pred, vocab_size, number_dict\n",
    ")\n",
    "\n",
    "print('masked_tokens', masked_tokens)\n",
    "print('masked_pos', masked_pos)\n",
    "print('input_ids', input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(input_ids, segment_ids, masked_tokens, masked_pos, max_len, max_pred):\n",
    "    n_pad = max_len - len(input_ids)\n",
    "    input_ids.extend([0] * n_pad)\n",
    "    segment_ids.extend([0] * n_pad)\n",
    "\n",
    "    if max_pred > len(masked_tokens):\n",
    "        n_pad = max_pred - len(masked_tokens)\n",
    "        masked_tokens.extend([0] * n_pad)\n",
    "        masked_pos.extend([0] * n_pad)\n",
    "\n",
    "    return input_ids, segment_ids, masked_tokens, masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos = pad_sequences(\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, max_len, max_pred\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_positive_example(tokens_a_index, tokens_b_index):\n",
    "    return tokens_a_index + 1 == tokens_b_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences, token_list, word_dict, number_dict, batch_size, max_len,\n",
    "               max_pred, vocab_size):\n",
    "    positive = negative = 0\n",
    "    batch = []\n",
    "\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        tokens_a, tokens_b, tokens_a_index, tokens_b_index = \\\n",
    "        select_random_sentence_pair(sentences, token_list)\n",
    "\n",
    "        input_ids, segment_ids = construct_input_segment_ids(tokens_a, tokens_b, word_dict)\n",
    "\n",
    "        masked_tokens, masked_pos, input_ids = mask_tokens(\n",
    "            input_ids, word_dict, max_pred, vocab_size, number_dict\n",
    "        )\n",
    "\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos = pad_sequences(\n",
    "            input_ids, segment_ids, masked_tokens, masked_pos, max_len, max_pred\n",
    "        )\n",
    "\n",
    "        if is_positive_example(tokens_a_index, tokens_b_index) and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif not is_positive_example(tokens_a_index, tokens_b_index) and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch(sentences, token_list, word_dict, number_dict,\n",
    "                   batch_size, max_len, max_pred, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(* batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype = torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = get_attn_pad_mask(input_ids, input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.w_q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.w_k = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.w_v = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask):\n",
    "        residual = q\n",
    "        batch_size = q.size(0)\n",
    "        q_s = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.w_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.w_v(v).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "        scores = torch.matmul(q_s, k_s.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores.masked_fill_(attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1), -1e9)\n",
    "        attn_scores = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn_scores, v_s)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        output = self.fc(context)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads, d_k, d_v)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderLayer()\n",
    "\n",
    "\n",
    "emb = Embedding()\n",
    "\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attn_mask = get_attn_pad_mask(input_ids, segment_ids)\n",
    "\n",
    "\n",
    "mha_output = MultiHeadAttention(d_model, n_heads, d_k, d_v)(embeds, embeds, embeds, attn_mask)\n",
    "\n",
    "output, a = mha_output\n",
    "\n",
    "\n",
    "output = enc.forward(enc_inputs = embeds, enc_self_attn_mask = attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        #NSP\n",
    "\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias = False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        # here we choosing the 0 index \n",
    "        # because while attention mechanism computation\n",
    "        # cls token is containing all information about attention scores between words\n",
    "        \n",
    "        h_pooled = self.activ1(self.fc(output[:, 0]))\n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        # we expanding masked pos to be compatible for broadcasting\n",
    "\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "\n",
    "\n",
    "\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "        return logits_lm, logits_clsf\n",
    "    \n",
    "\n",
    "bert = BERT()\n",
    "\n",
    "bert.forward(input_ids = input_ids, segment_ids = segment_ids, masked_pos = masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert.parameters(), lr = 0.00001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_lm, logits_clsf = bert(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)\n",
    "\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext)\n",
    "    \n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch: [{}] cost: [{}]'.format(epoch, loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test_sentences = [\n",
    "    \"Romeo loves Juliet.\",\n",
    "    \"Juliet loves Romeo.\",\n",
    "    \"Romeo and Juliet are together.\"\n",
    "]\n",
    "\n",
    "test_token_list = []\n",
    "for sentence in test_sentences:\n",
    "    arr = [word_dict.get(s, word_dict['[MASK]']) for s in sentence.lower().split()]\n",
    "    test_token_list.append(arr)\n",
    "\n",
    "tokens_a, tokens_b, _, _ = select_random_sentence_pair(test_sentences, test_token_list)\n",
    "\n",
    "input_ids, segment_ids = construct_input_segment_ids(tokens_a, tokens_b, word_dict)\n",
    "\n",
    "masked_tokens, masked_pos, input_ids = mask_tokens(\n",
    "    input_ids, word_dict, max_pred, vocab_size, number_dict\n",
    ")\n",
    "\n",
    "input_ids, segment_ids, masked_tokens, masked_pos = pad_sequences(\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, max_len, max_pred\n",
    ")\n",
    "\n",
    "input_ids_tensor = torch.LongTensor([input_ids])\n",
    "segment_ids_tensor = torch.LongTensor([segment_ids])\n",
    "masked_pos_tensor = torch.LongTensor([masked_pos])\n",
    "\n",
    "logits_lm, logits_clsf = bert(input_ids=input_ids_tensor, segment_ids=segment_ids_tensor, masked_pos=masked_pos_tensor)\n",
    "\n",
    "predicted_tokens = logits_lm.data.max(2)[1].numpy()[0]\n",
    "predicted_masked_tokens = [number_dict[pos] for pos in predicted_tokens if pos != 0]\n",
    "\n",
    "predicted_isNext = logits_clsf.data.max(1)[1].numpy()[0]\n",
    "\n",
    "print(\"Original Sentences:\")\n",
    "print(\"Sentence A:\", \" \".join([number_dict[token] for token in tokens_a]))\n",
    "print(\"Sentence B:\", \" \".join([number_dict[token] for token in tokens_b]))\n",
    "print(\"\\nPredicted Masked Tokens:\", predicted_masked_tokens)\n",
    "print(\"Predicted isNext:\", \"True\" if predicted_isNext else \"False\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
